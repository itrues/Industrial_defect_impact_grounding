2024-09-13 12:58:50,563 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=4, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=16, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-13 12:58:59,554 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-13 12:58:59,554 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-13 12:59:09,376 train INFO: The size of dataset: train(1237)  val(310)
2024-09-13 12:59:22,225 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.123605  time: 0.535207
2024-09-13 13:13:00,799 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-13 13:13:14,666 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-13 13:13:14,666 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-13 13:13:17,801 train INFO: The size of dataset: train(1237)  val(310)
2024-09-13 13:13:39,407 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.004359  time: 0.062192
2024-09-13 13:16:59,977 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-13 13:17:07,260 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-13 13:17:07,260 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-13 13:17:24,445 train INFO: The size of dataset: train(1237)  val(310)
2024-09-13 13:17:44,267 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.003472  time: 0.057440
2024-09-13 13:19:33,993 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-13 13:19:42,597 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-13 13:19:42,598 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-13 13:19:51,531 train INFO: The size of dataset: train(1237)  val(310)
2024-09-13 13:20:11,201 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.003501  time: 0.057601
2024-09-13 15:06:45,148 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-13 15:06:52,334 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-13 15:06:52,334 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-13 15:06:54,474 train INFO: The size of dataset: train(1237)  val(310)
2024-09-13 15:07:14,396 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.003778  time: 0.057426
2024-09-13 15:14:55,151 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-13 15:15:02,508 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-13 15:15:02,508 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-13 15:15:04,627 train INFO: The size of dataset: train(1237)  val(310)
2024-09-13 15:15:24,182 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.003825  time: 0.056833
2024-09-13 15:25:34,623 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-13 15:25:42,782 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-13 15:25:42,782 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-13 15:25:44,905 train INFO: The size of dataset: train(1237)  val(310)
2024-09-13 15:26:04,573 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.004046  time: 0.057047
2024-09-14 01:31:44,257 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-14 01:31:51,233 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-14 01:31:51,233 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-14 01:31:53,323 train INFO: The size of dataset: train(1237)  val(310)
2024-09-14 01:32:12,638 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.003491  time: 0.056533
2024-09-14 01:33:26,118 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-14 01:33:32,895 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-14 01:33:32,895 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-14 01:33:35,036 train INFO: The size of dataset: train(1237)  val(310)
2024-09-14 01:33:54,464 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.003530  time: 0.057405
2024-09-14 01:35:42,742 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-14 01:35:50,213 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-14 01:35:50,213 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-14 01:35:52,302 train INFO: The size of dataset: train(1237)  val(310)
2024-09-14 01:36:11,561 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.003513  time: 0.056463
2024-09-14 01:37:55,055 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-14 01:38:02,061 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-14 01:38:02,061 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-14 01:38:04,138 train INFO: The size of dataset: train(1237)  val(310)
2024-09-14 01:38:23,079 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.003277  time: 0.055904
2024-09-14 01:40:31,216 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-14 01:40:38,333 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-14 01:40:38,334 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-14 01:40:40,465 train INFO: The size of dataset: train(1237)  val(310)
2024-09-14 01:40:59,853 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.003818  time: 0.056619
2024-09-14 01:46:57,415 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-14 01:47:04,267 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-14 01:47:04,267 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-14 01:47:06,393 train INFO: The size of dataset: train(1237)  val(310)
2024-09-14 01:47:26,873 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.004031  time: 0.058552
2024-09-14 01:48:32,024 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-14 01:48:38,922 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-14 01:48:38,922 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-14 01:48:41,026 train INFO: The size of dataset: train(1237)  val(310)
2024-09-14 01:49:01,570 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.004063  time: 0.058559
2024-09-14 01:50:43,558 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-14 01:50:50,538 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-14 01:50:50,539 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-14 01:50:52,677 train INFO: The size of dataset: train(1237)  val(310)
2024-09-14 01:51:12,555 train INFO: [Test accuracy]  Acc@0.50: 0.7290  Acc@0.55: 0.6839  Acc@0.60: 0.6452  Acc@0.65: 0.5613  Acc@0.70: 0.5000  Acc@0.75: 0.4323  Acc@0.80: 0.3613  Acc@0.85: 0.2742  Acc@0.90: 0.1258  Mean_iou: 0.6093  
[Test time]  data_time: 0.003520  time: 0.057442
2024-09-17 03:11:28,066 train INFO: Namespace(lr=0.0001, lr_backbone=1e-05, lr_vis_enc=1e-05, lr_bert=1e-05, batch_size=16, weight_decay=0.0001, epochs=120, lr_drop=60, clip_max_norm=0.1, checkpoint_step=5, checkpoint_latest=False, checkpoint_best=True, load_weights_path='pretrained_checkpoints/detr-r101-referit.pth', freeze_modules=['backbone', 'input_proj', 'trans_encoder', 'bert'], freeze_param_names=[], freeze_epochs=10, freeze_losses=[], backbone='resnet101', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=1, pre_norm=False, bert_model='bert-base-uncased', bert_token_mode='bert-base-uncased', bert_output_dim=768, bert_output_layers=4, max_query_len=40, aux_loss=True, loss_loc='loss_boxes', box_xyxy=False, drop_every_steps=30, reinit_weights_epoch=[20, 55, 75], reinit_weights_modules=[], distillation_temperature=2.0, distillation_alpha=2.0, distillation_scaled_weight=10.0, text_distillation=False, encoder_distillation=True, detection_head_distillation=True, decode_feature_distillation=False, accumulate_update_weights=False, update_ema_infer_teacher_model=False, update_ema_teacher_model=True, ema=False, custom_milestones=[60, 80, 90], ema_decay=0.9995, resume_teacher='', bbox_loss_coef=5, giou_loss_coef=2, other_loss_coefs={}, data_root='./data/', split_root='./split/data/', dataset='mvtec_anomaly_detection', defined_split='deblurred', test_split='val', img_size=640, cache_images=False, output_dir='results/', save_pred_path='', device='cuda', seed=42, resume='work_dirs/VLTVG_R101_gref_umd_knowledge_distillation_batch_4_epoch_120_emadecay_0.995/checkpoint_best_acc.pth', start_epoch=0, eval=True, num_workers=4, pin_memory=True, collate_fn='collate_fn', batch_size_val=1, batch_size_test=1, train_transforms=[{'type': 'RandomSelect', 'transforms1': {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}, 'transforms2': {'type': 'Compose', 'transforms': [{'type': 'RandomResize', 'sizes': [400, 500, 600], 'resize_long_side': False}, {'type': 'RandomSizeCrop', 'min_size': 384, 'max_size': 600, 'check_method': {'func': 'iou', 'iou_thres': 0.5}}, {'type': 'RandomResize', 'sizes': [448, 480, 512, 544, 576, 608, 640]}]}, 'p': 0.5}, {'type': 'ColorJitter', 'brightness': 0.4, 'contrast': 0.4, 'saturation': 0.4}, {'type': 'RandomHorizontalFlip'}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'aug_translate': True}], test_transforms=[{'type': 'RandomResize', 'sizes': [640], 'record_resize_info': True}, {'type': 'ToTensor', 'keys': []}, {'type': 'NormalizeAndPad', 'size': 640, 'center_place': True}], enable_batch_accum=False, world_size=1, dist_url='env://', config='configs/VLTVG_R101_flickr.py', model_config={'decoder': {'type': 'DecoderWithExtraEncoder', 'num_queries': 1, 'query_dim': 256, 'norm_dim': 256, 'return_intermediate': True, 'num_layers': 6, 'layer': {'type': 'MultiStageDecoderLayer', 'd_model': 256, 'dim_feedforward': 2048, 'dropout': 0.0, 'word_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img_feat_chunk_num': 2}, 'num_extra_layers': 1, 'extra_layer': {'type': 'DiscriminativeFeatEncLayer', 'd_model': 256, 'img_query_with_pos': False, 'img2text_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2textcond_attn_args': {'type': 'MultiheadAttention', 'embed_dim': 256, 'num_heads': 8, 'dropout': 0.1}, 'img2img_attn_args': {'type': 'MHAttentionRPE', 'd_model': 256, 'h': 8, 'dropout': 0.1, 'pos_x_range': [-20, 20], 'pos_y_range': [-20, 20], 'pos_index_offset': 20}, 'vl_verify': {'text_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'img_proj': {'input_dim': 256, 'hidden_dim': 256, 'output_dim': 256, 'num_layers': 1}, 'scale': 1.0, 'sigma': 0.5, 'pow': 2.0}}}}, rank=0, gpu=0, distributed=True, dist_backend='nccl')
2024-09-17 03:11:34,555 train INFO: The num of learnable parameters: backbone(42172416), vis_enc(8414976), bert(108891648), rest(10725126)
2024-09-17 03:11:34,555 train INFO: Check the whole parameters: 170204166 = 170204166
2024-09-17 03:11:36,577 train INFO: The size of dataset: train(1237)  val(310)
